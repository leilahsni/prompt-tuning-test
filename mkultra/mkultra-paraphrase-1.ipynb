{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openprompt\n",
    "#!pip install torch\n",
    "#!pip install scikit-learn\n",
    "#!pip install git+https://github.com/corolla-johnson/mkultra.git#egg=mkultra --log PIP_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast, Adafactor\n",
    "from mkultra.tuning import GPTNeoPromptTuningLM, GPT2PromptTuningLM\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "from mkultra.trainers import WorldInfoTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/catasaurus___csv/catasaurus--paraphrase-dataset2-124c70288202401c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34c7facd32e4f68a8d7582ee6764b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"catasaurus/paraphrase-dataset2\")\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "for inpt, target in zip(dataset['train']['input_text'][:1000], dataset['train']['target_text'][:1000]):\n",
    "    inputs.append(inpt)\n",
    "    targets.append(target)\n",
    "    \n",
    "data = {'input': inputs,\n",
    "        'target': targets}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# create csv file\n",
    "df.to_csv('paraphrase-dataset.csv')\n",
    "\n",
    "# create json file\n",
    "dictionary = {'dataset': []}\n",
    "for inpt, target in zip(df['input'], df['target']):\n",
    "    dictionary['dataset'].append({\"call\": f'Sentence : {inpt}\\nParaphrase : ', \"response\": target})\n",
    "\n",
    "with open('paraphrase-dataset.json','w') as outfile:\n",
    "    json.dump(dictionary, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt length: 21 tokens\n"
     ]
    }
   ],
   "source": [
    "#-----------------------#\n",
    "#  Training Parameters  #\n",
    "#-----------------------#\n",
    "\n",
    "# This decides the length of your soft prompt in tokens.\n",
    "# They will be initialized from the first n tokens of your dataset.\n",
    "n_tokens = 100\n",
    "\n",
    "# Set this to a string to start with a specific tokenized string.\n",
    "# Be aware of the number of tokens.\n",
    "initial_prompt = \"\"\"A paraphrase is a different way to say the same thing. Paraphrase the following sentence.\\n\"\"\"\n",
    "\n",
    "if initial_prompt is not None:\n",
    "    print(f\"Initial prompt length: {len(tokenizer.encode(initial_prompt))} tokens\")\n",
    "\n",
    "# Decide the length of your training blocks in tokens.\n",
    "# Safe sizes for gpt-neo-2.7B-halved:\n",
    "#  - 700 on a Colab T4 (16GB)\n",
    "#  - 400 on a Colab K80 (12GB)\n",
    "#  - 32 on a GTX1080 (8GB)\n",
    "# If it seems a bit small, don't worry!\n",
    "# Soft prompts can be moved forward in context for the best effect.\n",
    "block_size = 32\n",
    "\n",
    "# Name your soft prompt project.\n",
    "sp_name = 'prompt-tuning-paraphrase-1'\n",
    "\n",
    "# What's the name of model you'll be using?\n",
    "model_name = 'gpt2'\n",
    "\n",
    "# Specify the model directory or huggingface name.\n",
    "model_dir = \"gpt2\"\n",
    "\n",
    "model_type = 'gpt2'\n",
    "\n",
    "# Specify the path to the text file used for training.\n",
    "text_path = 'paraphrase-dataset.json'\n",
    "\n",
    "# Specify the project directory.\n",
    "project_dir = f\"./{sp_name}-{model_name}/\"\n",
    "\n",
    "# Checkpoint interval in steps.\n",
    "checkpoint_interval = 1\n",
    "\n",
    "# Evaluation interval in steps.\n",
    "eval_interval = 1\n",
    "\n",
    "# How many blocks to use for evaluation.\n",
    "eval_blocks = 20\n",
    "\n",
    "# Adafactor hyperparameters\n",
    "optimizer_params = {\n",
    "    # Fixed learning rate, recommend 1e-4 to 1e-3\n",
    "    \"lr\": 1e-3,\n",
    "    \n",
    "    # 1st momentum, recommend 0\n",
    "    \"beta1\": 0,\n",
    "\n",
    "    # 2nd momentum decay schedule, recommend -0.3 (lower is slower)\n",
    "    \"decay_rate\": -0.3,\n",
    "\n",
    "    # Weight decay, recommend 1e-2 (WI is sensitive to overfitting)\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \n",
    "    # Update scaling, recommend False\n",
    "    \"scale_parameter\": False,\n",
    "    \n",
    "    # Built-in LR scheduler, recommend False\n",
    "    \"relative_step\": False\n",
    "    }\n",
    "\n",
    "# Gradient accumulation steps.\n",
    "base_acc_steps = 30\n",
    "\n",
    "# Gradient accumulation schedule.\n",
    "# If '0', use a fixed gradient accumulation.\n",
    "acc_doubling_rate = 0\n",
    "\n",
    "# Stop training after this many evals without improvement.\n",
    "# If '0', don't stop early.\n",
    "plateau_steps = 10\n",
    "\n",
    "scheduler_params = {\n",
    "   \"num_warmup_steps\": 10,\n",
    "   \"num_cycles\": 4,\n",
    "   \"num_training_steps\": 240\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load model\n",
    "\n",
    "if 'model' not in globals():\n",
    "    if model_type == 'gpt2':\n",
    "        model = GPT2PromptTuningLM.from_pretrained(model_dir).half().to(\"cuda\")\n",
    "    elif model_type == 'gpt-neo':\n",
    "        model = GPTNeoPromptTuningLM.from_pretrained(model_dir).half().to(\"cuda\")\n",
    "    else:\n",
    "        raise \"Invalid model type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 13:21:42.678061: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-10 13:21:42.678095: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created project directory at ./prompt-tuning-paraphrase-1-gpt2/\n",
      "No checkpoints found\n"
     ]
    }
   ],
   "source": [
    "#@title Initialize project\n",
    "#@markdown This will load the latest checkpoint if the project directory already exists.\n",
    "\n",
    "filename_for_checkpoint = lambda step: f\"{sp_name}-{model_name}-step-{step}.json\"\n",
    "loaded_sp = None\n",
    "project_files = None\n",
    "\n",
    "# Look for existing project directory\n",
    "try:\n",
    "    os.makedirs(project_dir)\n",
    "    print(f\"Created project directory at {project_dir}\")\n",
    "except FileExistsError:\n",
    "    print(f\"Found project directory at {project_dir}\")\n",
    "\n",
    "# Look for existing checkpoints\n",
    "project_files = os.listdir(project_dir)\n",
    "if project_files is not None:\n",
    "    checkpoint_files = [check_file for check_file in project_files if ('-step-' in check_file) ]\n",
    "\n",
    "    if len(checkpoint_files) > 0:\n",
    "        highest_step = max([ int(check_file[check_file.rfind('-step-')+6:-5]) for check_file in checkpoint_files ])\n",
    "        loaded_sp = SoftPrompt.from_file( os.path.join(project_dir, filename_for_checkpoint(highest_step)) )\n",
    "        print(f\"Loading latest checkpoint: {highest_step}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt length: 21\n"
     ]
    }
   ],
   "source": [
    "#@title Initialize soft prompt in model\n",
    "#@markdown If a checkpoint is present, use that.\n",
    "if loaded_sp is None:\n",
    "    if initial_prompt is None:\n",
    "        model.initialize_soft_prompt(n_tokens=n_tokens)\n",
    "    else:\n",
    "        initial_sp = SoftPrompt.from_string(initial_prompt, model, tokenizer)\n",
    "        print(f\"Initial prompt length: {len(initial_sp)}\")\n",
    "        model.set_soft_prompt(initial_sp)\n",
    "\n",
    "    sp_step = 0\n",
    "    eval_loss = 100\n",
    "else:\n",
    "    model.set_soft_prompt(loaded_sp)\n",
    "    sp_step = loaded_sp._metadata['step']\n",
    "    eval_loss = loaded_sp._metadata['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paraphrase-dataset.json\") as file:\n",
    "    blocks = json.load(file)\n",
    "\n",
    "for block in blocks['dataset']:\n",
    "    block['call'] = tokenizer(str(block['call']), return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    block['response'] = tokenizer(str(block['response']), return_tensors=\"pt\").input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arranged_blocks = list()\n",
    "\n",
    "for block in blocks['dataset']:\n",
    "    call = block['call']\n",
    "    response = block['response']\n",
    "    ignore_len = call.shape[-1]\n",
    "\n",
    "    # Cat spacing and call first\n",
    "    input_ids = torch.cat([call, response], dim=1)\n",
    "    labels = torch.cat([torch.full((1,ignore_len),-100).to(model.device), response], dim=1)\n",
    "\n",
    "    arranged_blocks.append((input_ids, labels))\n",
    "\n",
    "random.shuffle(arranged_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adafactor hyperparameters\n",
    "optimizer_params = {\n",
    "    # Fixed learning rate, recommend 1e-4 to 1e-3\n",
    "    \"lr\": 1e-5,\n",
    "    \n",
    "    # 1st momentum, recommend 0\n",
    "    \"beta1\": 0,\n",
    "\n",
    "    # 2nd momentum decay schedule, recommend -0.3 (lower is slower)\n",
    "    \"decay_rate\": -0.3,\n",
    "\n",
    "    # Weight decay, recommend 1e-2 (WI is sensitive to overfitting)\n",
    "    \"weight_decay\": 1e-1,\n",
    "    \n",
    "    # Update scaling, recommend False\n",
    "    \"scale_parameter\": False,\n",
    "    \n",
    "    # Built-in LR scheduler, recommend False\n",
    "    \"relative_step\": False\n",
    "    }\n",
    "\n",
    "# Feed soft params to optimizer\n",
    "optimizer_params['params'] = [model.get_soft_params()]\n",
    "optimizer = Adafactor(**optimizer_params)\n",
    "optimizer.state['step'] = sp_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 3.672533203125\n",
      "Epoch 2 loss: 3.68101171875\n",
      "Epoch 3 loss: 3.66973828125\n",
      "Epoch 4 loss: 3.6799375\n",
      "Epoch 5 loss: 3.680640625\n",
      "Epoch 6 loss: 3.684552734375\n",
      "Epoch 7 loss: 3.677462890625\n",
      "Epoch 8 loss: 3.684361328125\n",
      "Epoch 9 loss: 3.690923828125\n",
      "Epoch 10 loss: 3.680396484375\n",
      "Epoch 11 loss: 3.684556640625\n",
      "Epoch 12 loss: 3.676228515625\n",
      "Epoch 13 loss: 3.672009765625\n",
      "Epoch 14 loss: 3.683947265625\n",
      "Epoch 15 loss: 3.68124609375\n",
      "Epoch 16 loss: 3.683029296875\n",
      "Epoch 17 loss: 3.67472265625\n",
      "Epoch 18 loss: 3.68812890625\n",
      "Epoch 19 loss: 3.679875\n",
      "Epoch 20 loss: 3.693619140625\n",
      "Epoch 21 loss: 3.676841796875\n",
      "Epoch 22 loss: 3.684478515625\n",
      "Epoch 23 loss: 3.68096875\n",
      "Epoch 24 loss: 3.687767578125\n",
      "Epoch 25 loss: 3.6808515625\n",
      "Epoch 26 loss: 3.67787109375\n",
      "Epoch 27 loss: 3.68660546875\n",
      "Epoch 28 loss: 3.68065234375\n",
      "Epoch 29 loss: 3.687333984375\n",
      "Epoch 30 loss: 3.68080859375\n",
      "Epoch 31 loss: 3.688228515625\n",
      "Epoch 32 loss: 3.686248046875\n",
      "Epoch 33 loss: 3.680291015625\n",
      "Epoch 34 loss: 3.676966796875\n",
      "Epoch 35 loss: 3.68576171875\n",
      "Epoch 36 loss: 3.678845703125\n",
      "Epoch 37 loss: 3.68229296875\n",
      "Epoch 38 loss: 3.68595703125\n",
      "Epoch 39 loss: 3.684509765625\n",
      "Epoch 40 loss: 3.6831015625\n",
      "Epoch 41 loss: 3.678029296875\n",
      "Epoch 42 loss: 3.682685546875\n",
      "Epoch 43 loss: 3.6760625\n",
      "Epoch 44 loss: 3.6892578125\n",
      "Epoch 45 loss: 3.678240234375\n",
      "Epoch 46 loss: 3.6903671875\n",
      "Epoch 47 loss: 3.68601171875\n",
      "Epoch 48 loss: 3.692134765625\n",
      "Epoch 49 loss: 3.68384765625\n",
      "Epoch 50 loss: 3.6758671875\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for i in range(50):\n",
    "    random.shuffle(arranged_blocks)\n",
    "\n",
    "    for input_ids, labels in arranged_blocks:\n",
    "        model(input_ids=input_ids, labels=labels).loss.backward()\n",
    "\n",
    "    # Always accumulate gradient for the entire dataset\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Evaluate\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in arranged_blocks:\n",
    "            eval_loss += model(input_ids=input_ids, labels=input_ids).loss.item()\n",
    "    eval_loss /= len(arranged_blocks)\n",
    "    print(f\"Epoch {i+1} loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence : What's the weather like today?\n",
      "Paraphrase: Â I know there are some clouds and heavy winds in my area. There isn't much wind to worry about right now, especially with a sunny day on this earth when it is at its best. I'll leave you all that up for later next\n"
     ]
    }
   ],
   "source": [
    "# Try generating with your model\n",
    "model.eval()\n",
    "\n",
    "test = \"Sentence : What's the weather like today ?\\nParaphrase: \"\n",
    "\n",
    "call = tokenizer(test, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    input_ids=call,\n",
    "    do_sample=True,\n",
    "    min_length=call.shape[-1] + 50,\n",
    "    max_length=call.shape[-1] + 50,\n",
    "    temperature=0.8,\n",
    "    tfs = 0.9,\n",
    "    repetition_penalty = 2.0\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mkultra-env]",
   "language": "python",
   "name": "conda-env-mkultra-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
